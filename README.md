# ST-CLIP: SpatioTemporal Context Modeling and Cross-Modal Prompting for Action Recognition

Large pretrained models applied to image data have showcased significant progress in visual representation learning. However, the direct application of pretrained image models to video data is insufficient for capturing spatiotemporal context. The challenge lies in effectively transferring image-modal knowledge from web-scale data to the video domain. To study cross-modal transfer learning of pretrained image models for video action recognition, we propose a novel ST-CLIP for efficient spatiotemporal context modeling. Specifically, ST-CLIP incorporates the lightweight Token Reasoning Unit (TokRea) into each encoder of Vision Transformer (ViT), which enables ViT, initially without temporal knowledge, to efficiently reason about temporal relations between video frames. On the other hand, aggregating unaligned spatial context from neighboring frames might introduce numerous irrelevant cues, thus leading to ineffective spatiotemporal context modeling. Toward this issue, we propose the mutual information objective that imposes supervision on knowledge extraction from neighboring frames by maximizing the task-relevant mutual information. Furthermore, unlike conventional methods that involve a linear classifier on the top of learned visual representations, we propose a cross-modal prompting mechanism. This mechanism constructs a joint visual-and-semantic embedding space, where visual and semantic representations are prompted with each other, such that the feature distance between associated videos and labels is disclosed. The proposed ST-CLIP can be flexibly adapted to the zero-shot learning setting. Extensive experiments and ablation studies highlight the competitiveness of ST-CLIP compared to state-of-the-art methods. The proposed model achieves impressive performance on fully-supervised experiments and zero-shot experiments.


# Details on training and testing will be provided soon.
